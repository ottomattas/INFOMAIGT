Q1: Explain the difference between reward and value in the context of the multi-armed bandit.
(1 point)

solution: The reward is what you get after you pulled a lever. 
The value of a lever is the expected reward.
Many of you wrote that the value is the average reward.
Note the important difference between taking the average and taking the expectation!
--------------------------------------------------------------------------------
Q2: Give a super simple example of a multiarmed bandit framework that is at least remotely related to sports. 
(2 points)

solution: For example, you decide on how many pull ups you are attempting to do each day.
The next day you are either exhausted or you gained strength.
that would be two different possible rewards.
This example assumes that your ability is somewhat static and does not change a lot over time.
This may be true for very advanced athletes over a short period of time.

I think this question was the biggest cuase for removed points.
There should be some repeatability.
For instance, many students wrote that in soccer you choose to which player to pass.
Obviously you cannot pass to any player at any time.

--------------------------------------------------------------------------------
Q3: What is a meant by a roll-out in the context of the lecture.
(1 point)

solution: Agents playing against each other until one of the agents won.
In the lecture, we described it with random agents, but other types of agents are also possible 
to do a roll-out.
I think most students understood the question correctly, but some students
had trouble expressing themselves correctly.
--------------------------------------------------------------------------------
Q4: What is the advantage of using optimistic initial values?
(1 point)

solution: At the beginning there is more exploration.
--------------------------------------------------------------------------------
Q5: Why does the optimistic value initialization spike in its expected performance
at the (k+1)-th iteration?
(1 point)

solution: At the (k+1)-th iteration the option that performed so far best, is likely to be picked.
--------------------------------------------------------------------------------
Q6: What is the motivation of the upper confidence bound?
(1 point)

solution: the choice of the arm should depend both on how good the option is but also on how many times it already has been explored.
Epsilon greedy does not take into account the number of explorations.
--------------------------------------------------------------------------------
Q7: Give a rough upper bound on the number of game states for Tic Tac Toe.
And a one line explanation.
(1 point)

solution: 3^{9} every position can be either empty, cross or circle, there are 9 positions.
There were some better bounds, but they missed an explanation.
9! was often named, I gave it 0.5 points.
--------------------------------------------------------------------------------
Q8: What is the maximum branching factor of Tic Tac Toe?
(In the naive way, where we do not consider symmetries.)
(1 point)

solution: It is nine at the beginning of the game.
I think some people have not read the question carefully.
--------------------------------------------------------------------------------
Q9: Name the four steps of Monte Carlo Tree Search.
(1 point)

solution: Selection, Expansion, Simulation, Backpropagation.
